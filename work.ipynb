{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abf0682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9661204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"data/test.csv\", index_col=0)\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092abfb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18d62d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['keyword', 'location'], inplace=True)\n",
    "test_df.drop(columns=['keyword', 'location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66b03213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target\n",
       "id                                                           \n",
       "1   Our Deeds are the Reason of this #earthquake M...       1\n",
       "4              Forest fire near La Ronge Sask. Canada       1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "786b91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Come ChatWords Which i Get from a Github Repository\n",
    "# Repository Link : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    #\"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    #\"LMAO\": \"Laugh My A.. Off\",\n",
    "    #\"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    #\"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d58515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "\n",
    "def preprocessing(df):\n",
    "    # duplicates\n",
    "    # df.drop_duplicates(subset=\"text\", keep='first', inplace=True) # there are also duplicates in training set, drop_duplicates make the loss higher\n",
    "    \n",
    "    # lower\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    df['text'] = df['text'].str.replace(r'http\\S+|www\\.\\S+|https\\S+', ' ', case=False, regex=True)\n",
    "    \n",
    "    # erasing punctuations\n",
    "    df['text'] = df['text'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Spelling issue\n",
    "    def Spelling_Correction(text):\n",
    "        text_blob = TextBlob(text)\n",
    "        #print('done')\n",
    "        return text_blob.correct().string\n",
    "\n",
    "    #df['text_blob'] = df['text'].apply(Spelling_Correction)\n",
    "\n",
    "    # erasing stop_words\n",
    "    stop_words = stopwords.words('english')\n",
    "    #df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    # remove HTML tags\n",
    "    def remove_tags(text):\n",
    "        return re.sub('<[^<]+?>', ' ', text)\n",
    "\n",
    "    df['text'] = df['text'].apply(remove_tags)\n",
    "\n",
    "    # Handling ChatsWords\n",
    "    def chat_conversion(text):\n",
    "        new_text = []\n",
    "        for i in text.split():\n",
    "            if i.upper() in chat_words:\n",
    "                new_text.append(chat_words[i.upper()].lower())\n",
    "                #print(f\"ChatWord found ! That was '{i}'\")\n",
    "\n",
    "            else:\n",
    "                new_text.append(i)\n",
    "        return \" \".join(new_text)\n",
    "\n",
    "    df['text'] = df['text'].apply(chat_conversion)\n",
    "\n",
    "    # Handling Emojis\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda x: emoji.demojize(x))\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace(\":copyright:\",\"\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f8af3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target\n",
       "id                                                           \n",
       "1   our deeds are the reason of this earthquake ma...       1\n",
       "4               forest fire near la ronge sask canada       1\n",
       "5   all residents asked to shelter in place are be...       1\n",
       "6   13000 people receive wildfires evacuation orde...       1\n",
       "7   just got sent this photo from ruby alaska as s...       1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = preprocessing(train_df)\n",
    "test_df = preprocessing(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "130a3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.loc[train_df['text'] != train_df['text_blob'], [\"text\", \"text_blob\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b16ba384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_df.loc[train_df['text'] != train_df['text_emoji'], [\"text\", \"text_emoji\"]][\"text\"][3123])\n",
    "#print(train_df.loc[train_df['text'] != train_df['text_emoji'], [\"text\", \"text_emoji\"]][\"text_emoji\"][3123])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde8fe6",
   "metadata": {},
   "source": [
    "love food fun malaysian prime minister najib razak confirmed that the aircraft debris found on rì©union isla\n",
    "\n",
    "love food fun malaysian prime minister najib razak confirmed that the aircraft debris found on rì:copyright:union isla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4e5f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49405f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target  \\\n",
       "id                                                              \n",
       "1   our deeds are the reason of this earthquake ma...       1   \n",
       "4               forest fire near la ronge sask canada       1   \n",
       "5   all residents asked to shelter in place are be...       1   \n",
       "6   13000 people receive wildfires evacuation orde...       1   \n",
       "7   just got sent this photo from ruby alaska as s...       1   \n",
       "\n",
       "                                       text_tokenized  \n",
       "id                                                     \n",
       "1   our deeds are the reason of this earthquake ma...  \n",
       "4               forest fire near la ronge sask canada  \n",
       "5   all residents asked to shelter in place are be...  \n",
       "6   13000 people receive wildfires evacuation orde...  \n",
       "7   just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assurez-vous de télécharger les ressources nécessaires\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Fonction pour tokenizer le texte\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Appliquer le tokenizer à chaque observation\n",
    "train_df['text_tokenized'] = train_df['text'].apply(tokenize_text)\n",
    "train_df['text_tokenized'] = train_df['text_tokenized'].apply(lambda x: ' '.join([word for word in x]))\n",
    "\n",
    "test_df['text_tokenized'] = test_df['text'].apply(tokenize_text)\n",
    "test_df['text_tokenized'] = test_df['text_tokenized'].apply(lambda x: ' '.join([word for word in x]))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdc643",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99948d",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "359ffe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_df['text_lemmatized'] = train_df['text_tokenized'].apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(word , pos='v') for word in x.split()]))\n",
    "test_df['text_lemmatized'] = test_df['text_tokenized'].apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(word , pos='v') for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc2f5dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>our deeds be the reason of this earthquake may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>all residents ask to shelter in place be be no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target  \\\n",
       "id                                                              \n",
       "1   our deeds are the reason of this earthquake ma...       1   \n",
       "4               forest fire near la ronge sask canada       1   \n",
       "5   all residents asked to shelter in place are be...       1   \n",
       "6   13000 people receive wildfires evacuation orde...       1   \n",
       "7   just got sent this photo from ruby alaska as s...       1   \n",
       "\n",
       "                                       text_tokenized  \\\n",
       "id                                                      \n",
       "1   our deeds are the reason of this earthquake ma...   \n",
       "4               forest fire near la ronge sask canada   \n",
       "5   all residents asked to shelter in place are be...   \n",
       "6   13000 people receive wildfires evacuation orde...   \n",
       "7   just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                      text_lemmatized  \n",
       "id                                                     \n",
       "1   our deeds be the reason of this earthquake may...  \n",
       "4               forest fire near la ronge sask canada  \n",
       "5   all residents ask to shelter in place be be no...  \n",
       "6   13000 people receive wildfires evacuation orde...  \n",
       "7   just get send this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517827a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733beb7",
   "metadata": {},
   "source": [
    "## Corpus and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afc37567",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lemmatized = list(train_df.columns).index('text_lemmatized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0933bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lenght of the Corpus is : 7613\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(len(train_df)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', train_df.iloc[i, index_lemmatized])\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "print(f'The Lenght of the Corpus is : {len(corpus)}') # in fact, count of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "553536c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Corpus is : 105383\n"
     ]
    }
   ],
   "source": [
    "# Total number of words in corpus\n",
    "# Initialize total_words counter\n",
    "total_words = 0\n",
    "\n",
    "# Iterate through each element in the corpus list\n",
    "for text in corpus:\n",
    "    # Split the text into words and update the total_words counter\n",
    "    total_words += len(text.split())\n",
    "\n",
    "# Print the total number of words\n",
    "print(f\"Total words in Corpus is : {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f4073c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for text in corpus:\n",
    "    words = text.split()\n",
    "    vocabulary.update(words)\n",
    "\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "13e96496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lenght of the Vocabulary  is : 14967\n"
     ]
    }
   ],
   "source": [
    "# Lenght of Vocab\n",
    "print(f'The Lenght of the Vocabulary  is : {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b1d8ef58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all residents ask to shelter in place be be notify by officer no other evacuation or shelter in place order be expect'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[2, index_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47baf962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'infest',\n",
       " 'curse',\n",
       " 'zepp',\n",
       " 'hieroglyphics',\n",
       " 'deccgovuk',\n",
       " 'australian',\n",
       " 'firewise',\n",
       " 'pardon',\n",
       " 'breed']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "629cd26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, element in enumerate(train_df['text_tokenized']):\n",
    "#    if 'choppergatebronwynbishopauspol' in element:\n",
    "#        print(idx, element)\n",
    "#        break\n",
    "#    else:\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee5cec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, element in enumerate(train_df['text']):\n",
    "#    if 'muhammad' in element:\n",
    "#        print(idx, element)\n",
    "#        break\n",
    "#    else:\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454619a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75642f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7570cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text_lemmatized\"])\n",
    "\n",
    "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text_lemmatized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6aa226c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 15915)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e29ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "clf = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7623a2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5828892616588525"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571a18e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
